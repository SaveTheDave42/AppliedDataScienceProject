{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Literal\n",
    "from httpx import AsyncClient, Response\n",
    "from parsel import Selector\n",
    "import mysql.connector\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cities to scrape\n",
    "cities_to_scrape = [\"zurich\", \"winterthur\", \"uster\", \"duebendorf\", \"dietikon\", \"wetzikon\", \"wallisellen\", \"rapperswil-jona\", \"pfaffikon\", \"meilen\", \"kloten\", \"hinwil\", \"horgen\", \"bassersdorf\", \"adliswil\", \"opfikon\", \"regensdorf\", \"brugg\", \"buelach\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All cities in switzerland\n",
    "cities_to_scrape = [\"zurich\", \"winterthur\", \"uster\", \"duebendorf\", \"dietikon\", \"wetzikon\", \"wallisellen\", \"rapperswil-jona\", \"pfaffikon\", \"meilen\", \"kloten\", \"hinwil\", \"horgen\", \"bassersdorf\", \"adliswil\", \"opfikon\", \"regensdorf\", \"brugg\", \"buelach\", \"dubendorf\", \"schlieren\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create async client instance and set headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncClient(\n",
    "    headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse listing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_next_data(response: Response) -> Dict:\n",
    "    \"\"\"Parse listing data from homegate search\"\"\"\n",
    "    selector = Selector(response.text)\n",
    "    next_data = selector.xpath(\"//script[contains(text(),'window.__INITIAL_STATE__')]/text()\").get()\n",
    "    if not next_data:\n",
    "        return {}\n",
    "    next_data_json = json.loads(next_data.strip(\"window.__INITIAL_STATE__=\"))\n",
    "    return next_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_search(city: str, query_type: Literal[\"rent\", \"buy\"] = \"buy\") -> List[Dict]:\n",
    "    \"\"\"Scrape listing data from homegate search pages for a specific city\"\"\"\n",
    "    url = f\"https://www.homegate.ch/{query_type}/real-estate/city-{city}/matching-list\"\n",
    "    first_page = await client.get(url)\n",
    "    data = parse_next_data(first_page)[\"resultList\"][\"search\"][\"fullSearch\"][\"result\"]\n",
    "    if data is None:\n",
    "        print(f\"No data found for {city}\")\n",
    "        return []\n",
    "    search_data = data[\"listings\"]\n",
    "    max_search_pages = data[\"pageCount\"]\n",
    "    print(f\"Scraped first search page for {city}, remaining ({max_search_pages} search pages)\")\n",
    "    other_pages = [client.get(url=str(first_page.url) + f\"?ep={page}\") for page in range(2, max_search_pages + 1)]\n",
    "    for response in asyncio.as_completed(other_pages):\n",
    "        data = parse_next_data(await response)\n",
    "        if data is not None:\n",
    "            search_data.extend(data[\"resultList\"][\"search\"][\"fullSearch\"][\"result\"][\"listings\"])\n",
    "            print(f\"Scraped {len(search_data)} property listings from {city} search\")\n",
    "        await asyncio.sleep(5)  # delay\n",
    "    return search_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    all_search_data = []\n",
    "    for city in cities_to_scrape:\n",
    "        city_search_data = await scrape_search(city, query_type=\"buy\")\n",
    "        all_search_data.extend(city_search_data)\n",
    "    \n",
    "    # Close client\n",
    "    await client.aclose()\n",
    "\n",
    "    json_string = json.dumps(all_search_data, indent=2)\n",
    "    json_data = json.loads(json_string)\n",
    "\n",
    "    # Specify the columns to include in the DataFrame\n",
    "    columns = ['id', 'listing']\n",
    "\n",
    "    # Extract the 'listing' data\n",
    "    listing_data = [item['listing'] for item in json_data]\n",
    "\n",
    "    # Extract the 'prices' data\n",
    "    prices_data = [{'id': item['id'], **item['listing']['prices']['buy'], **item['listing']['address'], **item['listing']['characteristics'], **item['listing']['address']['geoCoordinates']} for item in json_data]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(listing_data)\n",
    "    # Create the DataFrame\n",
    "    df_prices = pd.DataFrame(prices_data)\n",
    "    # Replace 'NaN' values with None\n",
    "    df_prices = df_prices.replace({np.nan: None})\n",
    "\n",
    "    # Set the index of the DataFrame to the 'id' column\n",
    "    df.set_index('id', inplace=True)\n",
    "    # print(df_prices)\n",
    "\n",
    "    #Write data to database\n",
    "    # Create a connection to the database\n",
    "    db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"user\",\n",
    "        password=\"password\",\n",
    "        port=\"3306\",\n",
    "        database=\"realestatepredictor\"\n",
    "    )\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = db.cursor()\n",
    "\n",
    "    # Convert the DataFrame to a list of dictionaries\n",
    "    data_dict = df_prices.to_dict('records')\n",
    "\n",
    "    # Clear the table before inserting the new data\n",
    "    cursor.execute(\"DELETE FROM homegate\")\n",
    "    db.commit()\n",
    "    print(\"All entries deleted from 'homegate'.\")\n",
    "\n",
    "    # Iterate over the list of dictionaries and insert each one into the MySQL table\n",
    "    for data in data_dict:\n",
    "        query = \"INSERT INTO homegate (homegateid, price, rooms, floor, livingSpace, street, latitude, longitude, locality, postalcode) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        values = (data['id'], data['price'], data['numberOfRooms'], data['floor'], data['livingSpace'], data['street'], data['latitude'], data['longitude'], data['locality'], data['postalCode'])\n",
    "        cursor.execute(query, values)\n",
    "\n",
    "    # Commit the transaction\n",
    "    db.commit()\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped first search page for zurich, remaining (14 search pages)\n",
      "Scraped 24 property listings from zurich search\n",
      "Scraped 44 property listings from zurich search\n",
      "Scraped 64 property listings from zurich search\n",
      "Scraped 84 property listings from zurich search\n",
      "Scraped 104 property listings from zurich search\n",
      "Scraped 124 property listings from zurich search\n",
      "Scraped 144 property listings from zurich search\n",
      "Scraped 164 property listings from zurich search\n",
      "Scraped 184 property listings from zurich search\n",
      "Scraped 204 property listings from zurich search\n",
      "Scraped 224 property listings from zurich search\n",
      "Scraped 244 property listings from zurich search\n",
      "Scraped 264 property listings from zurich search\n",
      "Scraped first search page for winterthur, remaining (5 search pages)\n",
      "Scraped 23 property listings from winterthur search\n",
      "Scraped 43 property listings from winterthur search\n",
      "Scraped 63 property listings from winterthur search\n",
      "Scraped 83 property listings from winterthur search\n",
      "Scraped first search page for uster, remaining (2 search pages)\n",
      "Scraped 32 property listings from uster search\n",
      "Scraped first search page for duebendorf, remaining (1 search pages)\n",
      "Scraped first search page for dietikon, remaining (2 search pages)\n",
      "Scraped 26 property listings from dietikon search\n",
      "No data found for wetzikon\n",
      "Scraped first search page for wallisellen, remaining (2 search pages)\n",
      "Scraped 25 property listings from wallisellen search\n",
      "No data found for rapperswil-jona\n",
      "No data found for pfaffikon\n",
      "Scraped first search page for meilen, remaining (2 search pages)\n",
      "Scraped 23 property listings from meilen search\n",
      "Scraped first search page for kloten, remaining (2 search pages)\n",
      "Scraped 32 property listings from kloten search\n",
      "Scraped first search page for hinwil, remaining (2 search pages)\n",
      "Scraped 24 property listings from hinwil search\n",
      "Scraped first search page for horgen, remaining (2 search pages)\n",
      "Scraped 38 property listings from horgen search\n",
      "Scraped first search page for bassersdorf, remaining (2 search pages)\n",
      "Scraped 25 property listings from bassersdorf search\n",
      "Scraped first search page for adliswil, remaining (1 search pages)\n",
      "Scraped first search page for opfikon, remaining (1 search pages)\n",
      "Scraped first search page for regensdorf, remaining (1 search pages)\n",
      "No data found for brugg\n",
      "Scraped first search page for buelach, remaining (2 search pages)\n",
      "Scraped 35 property listings from buelach search\n",
      "All entries deleted from 'homegate'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # asyncio.run(main())\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adspenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
